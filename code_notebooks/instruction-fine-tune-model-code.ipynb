{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport json\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    Trainer, \n    TrainingArguments,\n    get_linear_schedule_with_warmup\n)\nfrom sklearn.model_selection import train_test_split\nimport wandb\nimport time\nfrom datetime import timedelta","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:27:32.177758Z","iopub.execute_input":"2025-05-04T15:27:32.178044Z","iopub.status.idle":"2025-05-04T15:28:04.539971Z","shell.execute_reply.started":"2025-05-04T15:27:32.178016Z","shell.execute_reply":"2025-05-04T15:28:04.539414Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 15:27:46.885591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746372467.150378      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746372467.229406      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install re\nimport re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:30:25.768463Z","iopub.execute_input":"2025-05-04T15:30:25.768735Z","iopub.status.idle":"2025-05-04T15:30:28.192925Z","shell.execute_reply.started":"2025-05-04T15:30:25.768716Z","shell.execute_reply":"2025-05-04T15:30:28.192172Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Set environment variable to avoid tokenizers parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:19.202317Z","iopub.execute_input":"2025-05-04T15:28:19.202606Z","iopub.status.idle":"2025-05-04T15:28:19.206485Z","shell.execute_reply.started":"2025-05-04T15:28:19.202583Z","shell.execute_reply":"2025-05-04T15:28:19.205650Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU count: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:20.111103Z","iopub.execute_input":"2025-05-04T15:28:20.111725Z","iopub.status.idle":"2025-05-04T15:28:20.116240Z","shell.execute_reply.started":"2025-05-04T15:28:20.111702Z","shell.execute_reply":"2025-05-04T15:28:20.115680Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU count: 2\nGPU 0: Tesla T4\nMemory: 15.83 GB\nGPU 1: Tesla T4\nMemory: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Constants\nMAX_LENGTH = 512  # Reduced from 1024 to fit more examples in memory\nBATCH_SIZE = 4    # Reduced batch size to prevent OOM errors\nGRADIENT_ACCUMULATION_STEPS = 8  # Increase effective batch size\nEPOCHS = 3\nLEARNING_RATE = 2e-5\nWARMUP_STEPS = 100\nSAVE_STEPS = 500\nEVAL_STEPS = 500\nBASE_MODEL = \"distilgpt2\"  # Smaller model to fit in memory\nOUTPUT_DIR = \"./instruction_model_alpaca\"\nGITHUB_REPO = \"siddhamapple/instruction_tuned_model\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:22.413715Z","iopub.execute_input":"2025-05-04T15:28:22.414313Z","iopub.status.idle":"2025-05-04T15:28:22.418193Z","shell.execute_reply.started":"2025-05-04T15:28:22.414290Z","shell.execute_reply":"2025-05-04T15:28:22.417445Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Downloading and preparing the Alpaca dataset\ndef download_alpaca_dataset():\n    \"\"\"Download the full Alpaca dataset with 52K examples\"\"\"\n    print(\"Downloading Alpaca dataset...\")\n    \n    # Option 1: Direct download\n    !wget -q https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n    \n    # Option 2: If direct download fails, clone the repo\n    if not os.path.exists(\"alpaca_data.json\"):\n        !git clone https://github.com/tatsu-lab/stanford_alpaca.git\n        !cp stanford_alpaca/alpaca_data.json ./\n    \n    # Load the dataset\n    with open(\"alpaca_data.json\", \"r\") as f:\n        data = json.load(f)\n    \n    print(f\"Loaded {len(data)} examples from Alpaca dataset\")\n    return data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:24.949961Z","iopub.execute_input":"2025-05-04T15:28:24.950411Z","iopub.status.idle":"2025-05-04T15:28:24.956533Z","shell.execute_reply.started":"2025-05-04T15:28:24.950389Z","shell.execute_reply":"2025-05-04T15:28:24.955875Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Format instruction data using a better prompt template\ndef format_instruction(example):\n    \"\"\"Format instruction data with a clearer prompt template\"\"\"\n    instruction = example[\"instruction\"]\n    input_text = example.get(\"input\", \"\")\n    output = example[\"output\"]\n    \n    # Better prompt template with clear section markers\n    if input_text:\n        formatted_text = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}\"\"\"\n    else:\n        formatted_text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{output}\"\"\"\n    \n    return formatted_text","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:27.467730Z","iopub.execute_input":"2025-05-04T15:28:27.468259Z","iopub.status.idle":"2025-05-04T15:28:27.472436Z","shell.execute_reply.started":"2025-05-04T15:28:27.468234Z","shell.execute_reply":"2025-05-04T15:28:27.471661Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Custom dataset for instruction tuning\nclass InstructionDataset(Dataset):\n    def __init__(self, examples, tokenizer, max_length=512):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        formatted_text = format_instruction(example)\n        \n        # Tokenize with padding\n        encoding = self.tokenizer(\n            formatted_text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        input_ids = encoding[\"input_ids\"].squeeze()\n        attention_mask = encoding[\"attention_mask\"].squeeze()\n        \n        # Create labels (same as input_ids for causal language modeling)\n        labels = input_ids.clone()\n        \n        # Replace padding tokens in labels with -100 so they're ignored in loss calculation\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:27.827299Z","iopub.execute_input":"2025-05-04T15:28:27.827556Z","iopub.status.idle":"2025-05-04T15:28:27.833247Z","shell.execute_reply.started":"2025-05-04T15:28:27.827538Z","shell.execute_reply":"2025-05-04T15:28:27.832558Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Custom collator function to handle batching\ndef custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100):\n    \"\"\"Custom collate function for batching instruction examples\"\"\"\n    batch_max_length = max(len(item[\"input_ids\"]) for item in batch)\n    \n    input_ids_list = []\n    attention_mask_list = []\n    labels_list = []\n    \n    for item in batch:\n        input_ids = item[\"input_ids\"]\n        attention_mask = item[\"attention_mask\"]\n        labels = item[\"labels\"]\n        \n        # Pad sequences to the maximum length in this batch\n        padding_length = batch_max_length - len(input_ids)\n        \n        if padding_length > 0:\n            # Pad input_ids and attention_mask\n            input_ids = torch.cat([input_ids, torch.ones(padding_length, dtype=torch.long) * pad_token_id])\n            attention_mask = torch.cat([attention_mask, torch.zeros(padding_length, dtype=torch.long)])\n            \n            # Pad labels with ignore_index\n            labels = torch.cat([labels, torch.ones(padding_length, dtype=torch.long) * ignore_index])\n        \n        input_ids_list.append(input_ids)\n        attention_mask_list.append(attention_mask)\n        labels_list.append(labels)\n    \n    return {\n        \"input_ids\": torch.stack(input_ids_list),\n        \"attention_mask\": torch.stack(attention_mask_list),\n        \"labels\": torch.stack(labels_list)\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:30.099906Z","iopub.execute_input":"2025-05-04T15:28:30.100819Z","iopub.status.idle":"2025-05-04T15:28:30.107399Z","shell.execute_reply.started":"2025-05-04T15:28:30.100784Z","shell.execute_reply":"2025-05-04T15:28:30.106045Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Main training function\ndef train_on_alpaca():\n    \"\"\"Train the instruction model on the full Alpaca dataset\"\"\"\n    start_time = time.time()\n    \n    # Download and prepare dataset\n    data = download_alpaca_dataset()\n    \n    # Initialize tokenizer and model\n    print(f\"Loading base model: {BASE_MODEL}\")\n    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n    \n    # Ensure the tokenizer has a pad token\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = model.config.eos_token_id\n    \n    # Split data into train and validation sets\n    train_data, val_data = train_test_split(data, test_size=0.05, random_state=42)\n    print(f\"Training on {len(train_data)} examples, validating on {len(val_data)} examples\")\n    \n    # Create datasets\n    train_dataset = InstructionDataset(train_data, tokenizer, max_length=MAX_LENGTH)\n    val_dataset = InstructionDataset(val_data, tokenizer, max_length=MAX_LENGTH)\n    \n    # Create data loaders with custom collation\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=BATCH_SIZE,\n        shuffle=True,\n        collate_fn=lambda batch: custom_collate_fn(batch, pad_token_id=tokenizer.pad_token_id)\n    )\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=BATCH_SIZE,\n        collate_fn=lambda batch: custom_collate_fn(batch, pad_token_id=tokenizer.pad_token_id)\n    )\n    \n    # Set up training arguments\n    training_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_steps=WARMUP_STEPS,\n    logging_dir=\"./logs\",\n    logging_steps=100,\n    save_steps=SAVE_STEPS,\n    \n    save_total_limit=2,\n    \n    fp16=True,  \n    report_to=\"none\",\n   \n)\n    \n    # Initialize Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=lambda batch: custom_collate_fn(batch, pad_token_id=tokenizer.pad_token_id)\n    )\n    \n    # Train the model\n    print(\"Starting training...\")\n    trainer.train()\n    \n    # Save the final model\n    print(\"Saving final model...\")\n    trainer.save_model(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    \n    # Calculate training time\n    training_time = time.time() - start_time\n    print(f\"Training completed in {timedelta(seconds=int(training_time))}\")\n    \n    return model, tokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:32.010676Z","iopub.execute_input":"2025-05-04T15:28:32.011261Z","iopub.status.idle":"2025-05-04T15:28:32.019204Z","shell.execute_reply.started":"2025-05-04T15:28:32.011239Z","shell.execute_reply":"2025-05-04T15:28:32.018587Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Push model to GitHub\ndef push_to_github():\n    \"\"\"Push the trained model to GitHub\"\"\"\n    print(\"Pushing model to GitHub...\")\n    \n    # Configure Git\n    !git config --global user.email \"siddhamjainn@gmail.com\"\n    !git config --global user.name \"siddhamapple\"\n    \n    # Clone repository if it doesn't exist\n    if not os.path.exists(GITHUB_REPO.split('/')[-1]):\n        !git clone https://github.com/{GITHUB_REPO}.git\n    \n    # Copy model files to the repository\n    repo_dir = GITHUB_REPO.split('/')[-1]\n    !mkdir -p {repo_dir}/models_alpaca\n    !cp -r {OUTPUT_DIR}/* {repo_dir}/models_alpaca/\n    \n    # Add, commit, and push\n    !cd {repo_dir} && git add .\n    !cd {repo_dir} && git commit -m \"Add Alpaca-trained instruction model\"\n    \n    # Use token-based authentication for pushing\n    # Note: Replace YOUR_GITHUB_TOKEN with your actual token in a real environment\n    !cd {repo_dir} && git push https://siddhamapple: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@github.com/{GITHUB_REPO}.git main\n    \n    # For security, it's better to use environment variables for tokens\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if github_token:\n        !cd {repo_dir} && git push https://siddhamapple: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@github.com/{GITHUB_REPO}.git main\n    else:\n        print(\"GitHub token not found. Please push manually or set the GITHUB_TOKEN environment variable.\")\n    \n    print(\"Model pushed to GitHub!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:28:35.969297Z","iopub.execute_input":"2025-05-04T15:28:35.970074Z","iopub.status.idle":"2025-05-04T15:28:35.991272Z","shell.execute_reply.started":"2025-05-04T15:28:35.970039Z","shell.execute_reply":"2025-05-04T15:28:35.990391Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Enhanced evaluation function with TF-IDF fallback\ndef evaluate_model_with_fallback(model, tokenizer, test_examples):\n    \"\"\"Evaluate the model with enhanced post-processing\"\"\"\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from nltk.corpus import stopwords\n    \n    # Download stopwords if needed\n    try:\n        stopwords.words('english')\n    except:\n        nltk.download('stopwords')\n    # TF-IDF fallback for keyword extraction\n    def extract_keywords_tfidf(text, num_keywords=5):\n        \"\"\"Extract keywords using TF-IDF when model output is unreliable\"\"\"\n        # Clean text\n        text = re.sub(r'[^\\w\\s]', '', text.lower())\n        \n        # Initialize TF-IDF vectorizer\n        stop_words = set(stopwords.words('english'))\n        vectorizer = TfidfVectorizer(\n            stop_words='english',\n            max_features=100,\n            ngram_range=(1, 2)  # Consider both unigrams and bigrams\n        )\n        \n        # Fit and transform the text\n        try:\n            tfidf_matrix = vectorizer.fit_transform([text])\n            feature_names = vectorizer.get_feature_names_out()\n            \n            # Get top keywords based on TF-IDF scores\n            tfidf_scores = zip(feature_names, tfidf_matrix.toarray()[0])\n            sorted_keywords = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n            \n            # Return top N keywords\n            return [keyword for keyword, score in sorted_keywords[:num_keywords]]\n        except:\n            # Fallback to simple frequency-based extraction if TF-IDF fails\n            words = text.split()\n            word_freq = {}\n            for word in words:\n                if word not in stop_words and len(word) > 3:\n                    word_freq[word] = word_freq.get(word, 0) + 1\n            \n            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n            return [word for word, _ in sorted_words[:num_keywords]]\n    \n    # Check if model output is valid\n    def is_valid_keyword_response(response, expected_count):\n        \"\"\"Check if the response looks like a proper keyword list\"\"\"\n        # Look for patterns like \"Keywords: x, y, z\" or a list of words\n        if \"keywords:\" in response.lower():\n            keyword_text = response.lower().split(\"keywords:\")[1].strip()\n            keywords = [k.strip() for k in re.split(r',|\\n', keyword_text) if k.strip()]\n            \n            # Check if we have a reasonable number of keywords\n            if len(keywords) >= max(1, expected_count * 0.5):\n                return True\n        \n        # Check for repetitive patterns (a sign of model hallucination)\n        words = response.split()\n        if len(words) > 10:\n            # Check for excessive repetition\n            word_set = set(words)\n            if len(word_set) < len(words) * 0.4:  # High repetition\n                return False\n        \n        return False\n    \n    # Process instruction with enhanced post-processing\n    def process_instruction_enhanced(text, instruction, num_items=5):\n        \"\"\"Process instruction with fallback mechanisms\"\"\"\n        # Generate response\n        prompt = f\"\"\"\nBelow is a blog post. {instruction}\n\nBlog: {text}\n\nResponse:\n\"\"\"\n        \n        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        \n        # Generate with parameters to avoid repetition\n        output = model.generate(\n            **inputs,\n            max_length=len(inputs[\"input_ids\"][0]) + 512,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True,\n            repetition_penalty=1.5,  # Increased repetition penalty\n            no_repeat_ngram_size=3,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        \n        response = tokenizer.decode(output[0], skip_special_tokens=True)\n        response_text = response.split(\"Response:\")[-1].strip()\n        \n        # For keyword extraction tasks, validate and potentially use the fallback\n        if \"keywords\" in instruction.lower():\n            if not is_valid_keyword_response(response_text, num_items):\n                # Use TF-IDF fallback\n                keywords = extract_keywords_tfidf(text, num_items)\n                return \"Keywords: \" + \", \".join(keywords)\n        \n        # Post-process to remove repetitions\n        response_text = re.sub(r'(.{30,}?)\\1+', r'\\1', response_text)\n        \n        return response_text\n    \n    # Evaluate on test examples\n    results = []\n    for example in test_examples[:10]:  # Evaluate on a subset for demonstration\n        instruction = example[\"instruction\"]\n        input_text = example.get(\"input\", \"\")\n        expected_output = example[\"output\"]\n        \n        # Extract number from instruction if present\n        num_items = 3  # Default\n        match = re.search(r'(\\d+)', instruction)\n        if match:\n            num_items = int(match.group(1))\n        \n        # Process with enhanced handling\n        if input_text:\n            full_text = f\"{instruction}\\n{input_text}\"\n        else:\n            full_text = instruction\n            \n        model_output = process_instruction_enhanced(full_text, instruction, num_items)\n        \n        results.append({\n            \"instruction\": instruction,\n            \"input\": input_text,\n            \"expected\": expected_output,\n            \"generated\": model_output\n        })\n    \n    return results\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T15:29:31.273452Z","iopub.execute_input":"2025-05-04T15:29:31.274359Z","iopub.status.idle":"2025-05-04T15:29:31.289142Z","shell.execute_reply.started":"2025-05-04T15:29:31.274319Z","shell.execute_reply":"2025-05-04T15:29:31.288504Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Main execution\nif __name__ == \"__main__\":\n    # Train the model\n    model, tokenizer = train_on_alpaca()\n    \n    # Push to GitHub\n    push_to_github()\n    \n    # Optional: Evaluate the model\n    # Load the Alpaca dataset for evaluation\n    with open(\"alpaca_data.json\", \"r\") as f:\n        eval_data = json.load(f)\n    \n    # Use a small subset for evaluation\n    eval_results = evaluate_model_with_fallback(model, tokenizer, eval_data[:100])\n    \n    # Print some evaluation results\n    print(\"\\nEvaluation Results:\")\n    for i, result in enumerate(eval_results[:3]):\n        print(f\"\\nExample {i+1}:\")\n        print(f\"Instruction: {result['instruction']}\")\n        if result['input']:\n            print(f\"Input: {result['input']}\")\n        print(f\"Expected: {result['expected']}\")\n        print(f\"Generated: {result['generated']}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T18:58:10.991257Z","iopub.execute_input":"2025-05-03T18:58:10.991950Z","iopub.status.idle":"2025-05-03T20:50:31.416783Z","shell.execute_reply.started":"2025-05-03T18:58:10.991924Z","shell.execute_reply":"2025-05-03T20:50:31.415707Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU count: 2\nGPU 0: Tesla T4\nMemory: 15.83 GB\nGPU 1: Tesla T4\nMemory: 15.83 GB\nDownloading Alpaca dataset...\nLoaded 52002 examples from Alpaca dataset\nLoading base model: distilgpt2\nTraining on 49401 examples, validating on 2601 examples\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3785684290.py:234: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2316' max='2316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2316/2316 1:51:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.357600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.967100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.967500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.924000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.938100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.920400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.899400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.896300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.898800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.909200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.889200</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.894500</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.903800</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.882900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.885300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.880600</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.878000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.883900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.885300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.887900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.872000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.881500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.874600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Saving final model...\nTraining completed in 1:51:19\nPushing model to GitHub...\nCloning into 'instruction_tuned_model'...\nremote: Enumerating objects: 27, done.\u001b[K\nremote: Counting objects: 100% (27/27), done.\u001b[K\nremote: Compressing objects: 100% (21/21), done.\u001b[K\nremote: Total 27 (delta 5), reused 27 (delta 5), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (27/27), 1.14 MiB | 8.41 MiB/s, done.\nResolving deltas: 100% (5/5), done.\nFiltering content: 100% (2/2), 566.43 MiB | 127.73 MiB/s, done.\n[main b1fca40] Add Alpaca-trained instruction model\n 37 files changed, 901608 insertions(+)\n create mode 100644 models_alpaca/checkpoint-2000/config.json\n create mode 100644 models_alpaca/checkpoint-2000/generation_config.json\n create mode 100644 models_alpaca/checkpoint-2000/merges.txt\n create mode 100644 models_alpaca/checkpoint-2000/model.safetensors\n create mode 100644 models_alpaca/checkpoint-2000/optimizer.pt\n create mode 100644 models_alpaca/checkpoint-2000/rng_state.pth\n create mode 100644 models_alpaca/checkpoint-2000/scaler.pt\n create mode 100644 models_alpaca/checkpoint-2000/scheduler.pt\n create mode 100644 models_alpaca/checkpoint-2000/special_tokens_map.json\n create mode 100644 models_alpaca/checkpoint-2000/tokenizer.json\n create mode 100644 models_alpaca/checkpoint-2000/tokenizer_config.json\n create mode 100644 models_alpaca/checkpoint-2000/trainer_state.json\n create mode 100644 models_alpaca/checkpoint-2000/training_args.bin\n create mode 100644 models_alpaca/checkpoint-2000/vocab.json\n create mode 100644 models_alpaca/checkpoint-2316/config.json\n create mode 100644 models_alpaca/checkpoint-2316/generation_config.json\n create mode 100644 models_alpaca/checkpoint-2316/merges.txt\n create mode 100644 models_alpaca/checkpoint-2316/model.safetensors\n create mode 100644 models_alpaca/checkpoint-2316/optimizer.pt\n create mode 100644 models_alpaca/checkpoint-2316/rng_state.pth\n create mode 100644 models_alpaca/checkpoint-2316/scaler.pt\n create mode 100644 models_alpaca/checkpoint-2316/scheduler.pt\n create mode 100644 models_alpaca/checkpoint-2316/special_tokens_map.json\n create mode 100644 models_alpaca/checkpoint-2316/tokenizer.json\n create mode 100644 models_alpaca/checkpoint-2316/tokenizer_config.json\n create mode 100644 models_alpaca/checkpoint-2316/trainer_state.json\n create mode 100644 models_alpaca/checkpoint-2316/training_args.bin\n create mode 100644 models_alpaca/checkpoint-2316/vocab.json\n create mode 100644 models_alpaca/config.json\n create mode 100644 models_alpaca/generation_config.json\n create mode 100644 models_alpaca/merges.txt\n create mode 100644 models_alpaca/model.safetensors\n create mode 100644 models_alpaca/special_tokens_map.json\n create mode 100644 models_alpaca/tokenizer.json\n create mode 100644 models_alpaca/tokenizer_config.json\n create mode 100644 models_alpaca/training_args.bin\n create mode 100644 models_alpaca/vocab.json\nUploading LFS objects: 100% (8/8), 2.0 GB | 60 MB/s, done.                      \nEnumerating objects: 25, done.\nCounting objects: 100% (25/25), done.\nDelta compression using up to 4 threads\nCompressing objects: 100% (24/24), done.\nWriting objects: 100% (24/24), 1.15 MiB | 3.93 MiB/s, done.\nTotal 24 (delta 5), reused 7 (delta 0), pack-reused 0\nremote: Resolving deltas: 100% (5/5), completed with 1 local object.\u001b[K\nTo https://github.com/siddhamapple/instruction_tuned_model.git\n   bb9f5dc..b1fca40  main -> main\nGitHub token not found. Please push manually or set the GITHUB_TOKEN environment variable.\nModel pushed to GitHub!\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/3785684290.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Use a small subset for evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Print some evaluation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/3785684290.py\u001b[0m in \u001b[0;36mevaluate_model_with_fallback\u001b[0;34m(model, tokenizer, test_examples)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# Extract number from instruction if present\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mnum_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m  \u001b[0;31m# Default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'(\\d+)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mnum_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 're' is not defined"],"ename":"NameError","evalue":"name 're' is not defined","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}